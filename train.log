wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.
wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.
wandb: Appending key for api.wandb.ai to your netrc file: /users/biocomp/mahanta/.netrc
wandb: Currently logged in as: saranga7 to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]Loading pipeline components...:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 4/6 [00:00<00:00, 30.15it/s]Loading pipeline components...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 32.64it/s]
You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .
/import/pr_deepdevpath2/Saranga/Transfer_T2I_Task2/.venv/lib/python3.11/site-packages/lightning/fabric/connector.py:571: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!
Using bfloat16 Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA L40S') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
wandb: Tracking run with wandb version 0.21.1
wandb: Run data is saved locally in ./wandb/run-20250825_212754-l9abm34r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earthy-aardvark-18
wandb: ‚≠êÔ∏è View project at https://wandb.ai/saranga7/vangogh
wandb: üöÄ View run at https://wandb.ai/saranga7/vangogh/runs/l9abm34r
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
/import/pr_deepdevpath2/Saranga/Transfer_T2I_Task2/.venv/lib/python3.11/site-packages/lightning/pytorch/utilities/model_summary/model_summary.py:231: Precision bf16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.

  | Name         | Type          | Params | Mode 
-------------------------------------------------------
0 | unet         | PeftModel     | 860 M  | train
1 | vae          | AutoencoderKL | 83.7 M | eval 
2 | text_encoder | CLIPTextModel | 123 M  | eval 
-------------------------------------------------------
207 M     Trainable params
859 M     Non-trainable params
1.1 B     Total params
4,268.130 Total estimated model params size (MB)
1282      Modules in train mode
1104      Modules in eval mode
/import/pr_deepdevpath2/Saranga/Transfer_T2I_Task2/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/4 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/4 [00:00<?, ?it/s] Epoch 0:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:02<00:06,  0.49it/s]Epoch 0:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:02<00:06,  0.49it/s, v_num=m34r]Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:02<00:02,  0.79it/s, v_num=m34r]Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:02<00:02,  0.79it/s, v_num=m34r]Epoch 0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.10it/s, v_num=m34r]Epoch 0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.10it/s, v_num=m34r]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.01it/s, v_num=m34r]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.01it/s, v_num=m34r]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.01it/s, v_num=m34r]Epoch 0:   0%|          | 0/4 [00:00<?, ?it/s, v_num=m34r]        Epoch 1:   0%|          | 0/4 [00:00<?, ?it/s, v_num=m34r]Epoch 1:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:01,  1.95it/s, v_num=m34r]Epoch 1:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:01,  1.95it/s, v_num=m34r]Epoch 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:00<00:00,  2.25it/s, v_num=m34r]Epoch 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:00<00:00,  2.25it/s, v_num=m34r]Epoch 1:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:01<00:00,  2.39it/s, v_num=m34r]Epoch 1:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:01<00:00,  2.39it/s, v_num=m34r]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.51it/s, v_num=m34r]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.51it/s, v_num=m34r]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.51it/s, v_num=m34r]Epoch 1:   0%|          | 0/4 [00:00<?, ?it/s, v_num=m34r]        Epoch 2:   0%|          | 0/4 [00:00<?, ?it/s, v_num=m34r]Epoch 2:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:01,  1.88it/s, v_num=m34r]Epoch 2:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:01,  1.87it/s, v_num=m34r]Epoch 2:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:00<00:00,  2.20it/s, v_num=m34r]Epoch 2:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:00<00:00,  2.20it/s, v_num=m34r]Epoch 2:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:01<00:00,  2.35it/s, v_num=m34r]Epoch 2:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:01<00:00,  2.35it/s, v_num=m34r]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.48it/s, v_num=m34r]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.48it/s, v_num=m34r]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.47it/s, v_num=m34r]Epoch 2:   0%|          | 0/4 [00:00<?, ?it/s, v_num=m34r]        Epoch 3:   0%|          | 0/4 [00:00<?, ?it/s, v_num=m34r]Epoch 3:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:01,  1.65it/s, v_num=m34r]Epoch 3:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:01,  1.65it/s, v_num=m34r]Epoch 3:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:00<00:00,  2.04it/s, v_num=m34r]Epoch 3:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:00<00:00,  2.04it/s, v_num=m34r]Epoch 3:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:01<00:00,  2.23it/s, v_num=m34r]Epoch 3:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:01<00:00,  2.23it/s, v_num=m34r]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.38it/s, v_num=m34r]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.38it/s, v_num=m34r]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.37it/s, v_num=m34r]Epoch 3:   0%|          | 0/4 [00:00<?, ?it/s, v_num=m34r]        Epoch 4:   0%|          | 0/4 [00:00<?, ?it/s, v_num=m34r]Epoch 4:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:01,  1.90it/s, v_num=m34r]Epoch 4:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:01,  1.90it/s, v_num=m34r]Epoch 4:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:00<00:00,  2.22it/s, v_num=m34r]Epoch 4:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:00<00:00,  2.21it/s, v_num=m34r]Epoch 4:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:01<00:00,  2.36it/s, v_num=m34r]Epoch 4:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:01<00:00,  2.36it/s, v_num=m34r]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.49it/s, v_num=m34r]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.49it/s, v_num=m34r]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.49it/s, v_num=m34r]`Trainer.fit` stopped: `max_epochs=5` reached.
Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.49it/s, v_num=m34r]
[1;34mwandb[0m: 
[1;34mwandb[0m: üöÄ View run [33mearthy-aardvark-18[0m at: [34mhttps://wandb.ai/saranga7/vangogh/runs/l9abm34r[0m
